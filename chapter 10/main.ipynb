{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0911596",
   "metadata": {},
   "source": [
    "# <center>**Chapter 10 : Building Neural Networks with Pytorch**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4da94",
   "metadata": {},
   "source": [
    "## **PyTorch Fundamentals**\n",
    "**The core data structure of pytorch is a tensor. It's a multidimensional array with a shape and data type, used for numerical computations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e320d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6061ae8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0 , 4.0 , 7.0] , [2.0 , 3.0 , 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60c4df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e484e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor([4., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing in tensor works same as numpy arrays\n",
    "\n",
    "X[0 , 1], X[: , 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758a083e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 50., 80.],\n",
       "        [30., 40., 70.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (X + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f31d4",
   "metadata": {},
   "source": [
    "**you can also convert a tensor to a numpy array using the ``numpy()``  method and create a tensor from a numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4242dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 7.],\n",
       "       [2., 3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e28e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1. , 4. , 7.] , [2. , 3. , 6.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02677c",
   "metadata": {},
   "source": [
    "**the default precision for floats in 32 bits in pytorch whereas its 64 bits in Numpy. Its generally better to use 32 bits in deep learning because this takes half the RAM and speeds up computations , and neural networks do not actually need the extra precision offered by 64 bit floats.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e5b14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can use ``torch.FloatTensor()`` which automatically converts the array to 32 bits\n",
    "\n",
    "torch.FloatTensor(np.array([[1. , 4. , 7.] , [2. ,3. , 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175384b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., -99.,   7.],\n",
       "        [  2., -99.,   6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also modify a tensor in place using indexing and slicing as with a numpy array\n",
    "\n",
    "X[: , 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3701a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 7.],\n",
       "        [2., 0., 6.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the relu method applies the ReLU activation function in place by replacing all negative values with 0s\n",
    "\n",
    "X.relu_()\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2d14e",
   "metadata": {},
   "source": [
    "#### **Tip : Pytorch's inplace operations are easy to spot at a glance because their name always ends with an underscore**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6c25e",
   "metadata": {},
   "source": [
    "## **Hardware Acceleration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29f4ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba34d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backend.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else :\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "febbf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1. , 2. , 3.] , [4. , 5. , 6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a48677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "457b7f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd9046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47 ms ± 159 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000))  # on the CPU\n",
    "%timeit M @M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af287159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548 μs ± 14 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000) , device = \"cuda\")  # on the GPU\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52c858",
   "metadata": {},
   "source": [
    "## **Autograd**\n",
    "**PyTorch comes with an efficient implementation of reverse mode auto differentation called autograd which stands for automatic gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acd51680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "f = x**2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34cce763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e86a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fed5bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971004bb",
   "metadata": {},
   "source": [
    "### **Warning : if you forget to zero out the gradients at each training iteration , the backward() method will just accumulate them, causing incorrect gradient updates. Since there wont be any explicit error, just low performance (and perhaps infinite or NaN values) , this issue may be hard to debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a792f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole traning looks like this \n",
    "\n",
    "learning_rate  = 0.1\n",
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()  # backward pass\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad   # gradient descent step\n",
    "\n",
    "    x.grad.zero_() # reset the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6913b827",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ExpBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m z = t.exp() \u001b[38;5;66;03m# this is an intermediate result\u001b[39;00m\n\u001b[32m      3\u001b[39m z += \u001b[32m1\u001b[39m \u001b[38;5;66;03m# this is an inplace operation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# runtime error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ExpBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "t = torch.tensor(2.0 , requires_grad=True)\n",
    "z = t.exp() # this is an intermediate result\n",
    "z += 1 # this is an inplace operation\n",
    "z.backward() # runtime error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f07e6",
   "metadata": {},
   "source": [
    "- **Some operations such as ``exp()`` , `relu()` , `rsqrt()` , `sigmoid()` , `sqrt()` , `tan()` , and `tanh()` save their outputs in the computation graph during the forward pass , then use these outputs to compute the gradients during the backward pass.This means that you must not modify such an operation's output in place, or you will get an error during the forward pass**\n",
    "\n",
    "- **Other operations such as `abs()` , `cos()` , `log()` , `sin()` , `square()` , and `var()` save their inputs instead of their output. Such an operation doesnot care if you modify its output in place , but you must not modify its input in place before the backward pass.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84478b",
   "metadata": {},
   "source": [
    "## **Implementing Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfde653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10d7cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data , housing.target , random_state = 42)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full , y_train_full , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f7518d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim = 0 , keepdim=True)\n",
    "stds = X_train.std(dim = 0 , keepdim=True)\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds\n",
    "X_test = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbd43de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets convert the targerts to tensors too\n",
    "\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_valid = torch.FloatTensor(y_valid)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89c7b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the parameters of our linear regeression model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_features = X_train.shape[1] # there are 8 input features\n",
    "w = torch.randn((n_features , 1), requires_grad = True)\n",
    "b = torch.tensor(0. , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94793e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss : 16.235748291015625\n",
      "Epoch 2/20, Loss : 5.34673547744751\n",
      "Epoch 3/20, Loss : 2.8323731422424316\n",
      "Epoch 4/20, Loss : 1.944043517112732\n",
      "Epoch 5/20, Loss : 1.6030147075653076\n",
      "Epoch 6/20, Loss : 1.4679163694381714\n",
      "Epoch 7/20, Loss : 1.4126158952713013\n",
      "Epoch 8/20, Loss : 1.388764500617981\n",
      "Epoch 9/20, Loss : 1.377511739730835\n",
      "Epoch 10/20, Loss : 1.3714261054992676\n",
      "Epoch 11/20, Loss : 1.3675490617752075\n",
      "Epoch 12/20, Loss : 1.3646897077560425\n",
      "Epoch 13/20, Loss : 1.3623602390289307\n",
      "Epoch 14/20, Loss : 1.3603529930114746\n",
      "Epoch 15/20, Loss : 1.3585747480392456\n",
      "Epoch 16/20, Loss : 1.3569780588150024\n",
      "Epoch 17/20, Loss : 1.3555351495742798\n",
      "Epoch 18/20, Loss : 1.35422682762146\n",
      "Epoch 19/20, Loss : 1.3530385494232178\n",
      "Epoch 20/20, Loss : 1.35195791721344\n"
     ]
    }
   ],
   "source": [
    "# we will use batch gradient descent (BGD) , using the full training set at each training step\n",
    "\n",
    "learning_rate = 0.4\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = X_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        b -= learning_rate * b.grad\n",
    "        w -= learning_rate * w.grad\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e3c37d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1702],\n",
       "        [2.0141],\n",
       "        [2.0942]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3] # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = X_new @ w + b # use the trained parameters to make predictions\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea35b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
