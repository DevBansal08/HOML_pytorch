{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0911596",
   "metadata": {},
   "source": [
    "# <center>**Chapter 10 : Building Neural Networks with Pytorch**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4da94",
   "metadata": {},
   "source": [
    "## **PyTorch Fundamentals**\n",
    "**The core data structure of pytorch is a tensor. It's a multidimensional array with a shape and data type, used for numerical computations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e320d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6061ae8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0 , 4.0 , 7.0] , [2.0 , 3.0 , 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60c4df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e484e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor([4., 3.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing in tensor works same as numpy arrays\n",
    "\n",
    "X[0 , 1], X[: , 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758a083e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 50., 80.],\n",
       "        [30., 40., 70.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (X + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f31d4",
   "metadata": {},
   "source": [
    "**you can also convert a tensor to a numpy array using the ``numpy()``  method and create a tensor from a numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4242dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 7.],\n",
       "       [2., 3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e28e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1. , 4. , 7.] , [2. , 3. , 6.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02677c",
   "metadata": {},
   "source": [
    "**the default precision for floats in 32 bits in pytorch whereas its 64 bits in Numpy. Its generally better to use 32 bits in deep learning because this takes half the RAM and speeds up computations , and neural networks do not actually need the extra precision offered by 64 bit floats.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e5b14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can use ``torch.FloatTensor()`` which automatically converts the array to 32 bits\n",
    "\n",
    "torch.FloatTensor(np.array([[1. , 4. , 7.] , [2. ,3. , 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175384b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., -99.,   7.],\n",
       "        [  2., -99.,   6.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also modify a tensor in place using indexing and slicing as with a numpy array\n",
    "\n",
    "X[: , 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3701a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 7.],\n",
       "        [2., 0., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the relu method applies the ReLU activation function in place by replacing all negative values with 0s\n",
    "\n",
    "X.relu_()\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2d14e",
   "metadata": {},
   "source": [
    "#### **Tip : Pytorch's inplace operations are easy to spot at a glance because their name always ends with an underscore**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6c25e",
   "metadata": {},
   "source": [
    "## **Hardware Acceleration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29f4ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba34d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backend.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else :\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "febbf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1. , 2. , 3.] , [4. , 5. , 6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a48677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "457b7f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd9046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.1 ms ± 530 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000))  # on the CPU\n",
    "%timeit M @M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af287159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09 ms ± 301 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000) , device = \"cuda\")  # on the GPU\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52c858",
   "metadata": {},
   "source": [
    "## **Autograd**\n",
    "**PyTorch comes with an efficient implementation of reverse mode auto differentation called autograd which stands for automatic gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acd51680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "f = x**2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34cce763",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e86a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.1\n",
    "# with torch.no_grad():\n",
    "#     x -= learning_rate * x.grad\n",
    "#     x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed5bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_detached = x.detach()\n",
    "# x_detached -= learning_rate * x.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60d4fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971004bb",
   "metadata": {},
   "source": [
    "### **Warning : if you forget to zero out the gradients at each training iteration , the backward() method will just accumulate them, causing incorrect gradient updates. Since there wont be any explicit error, just low performance (and perhaps infinite or NaN values) , this issue may be hard to debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a792f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole traning looks like this \n",
    "\n",
    "learning_rate  = 0.1\n",
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()  # backward pass\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad   # gradient descent step\n",
    "\n",
    "    x.grad.zero_() # reset the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6913b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(2.0 , requires_grad=True)\n",
    "z = t.exp() # this is an intermediate result\n",
    "z += 1 # this is an inplace operation\n",
    "# z.backward() # runtime error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f07e6",
   "metadata": {},
   "source": [
    "- **Some operations such as ``exp()`` , `relu()` , `rsqrt()` , `sigmoid()` , `sqrt()` , `tan()` , and `tanh()` save their outputs in the computation graph during the forward pass , then use these outputs to compute the gradients during the backward pass.This means that you must not modify such an operation's output in place, or you will get an error during the forward pass**\n",
    "\n",
    "- **Other operations such as `abs()` , `cos()` , `log()` , `sin()` , `square()` , and `var()` save their inputs instead of their output. Such an operation doesnot care if you modify its output in place , but you must not modify its input in place before the backward pass.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84478b",
   "metadata": {},
   "source": [
    "## **Implementing Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfde653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10d7cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data , housing.target , random_state = 42)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full , y_train_full , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f7518d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim = 0 , keepdim=True)\n",
    "stds = X_train.std(dim = 0 , keepdim=True)\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds\n",
    "X_test = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbd43de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets convert the targerts to tensors too\n",
    "\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_valid = torch.FloatTensor(y_valid)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89c7b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the parameters of our linear regeression model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_features = X_train.shape[1] # there are 8 input features\n",
    "w = torch.randn((n_features , 1), requires_grad = True)\n",
    "b = torch.tensor(0. , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94793e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss : 16.235748291015625\n",
      "Epoch 2/20, Loss : 5.34673547744751\n",
      "Epoch 3/20, Loss : 2.8323731422424316\n",
      "Epoch 4/20, Loss : 1.944043517112732\n",
      "Epoch 5/20, Loss : 1.6030147075653076\n",
      "Epoch 6/20, Loss : 1.4679163694381714\n",
      "Epoch 7/20, Loss : 1.4126158952713013\n",
      "Epoch 8/20, Loss : 1.388764500617981\n",
      "Epoch 9/20, Loss : 1.377511739730835\n",
      "Epoch 10/20, Loss : 1.3714261054992676\n",
      "Epoch 11/20, Loss : 1.3675490617752075\n",
      "Epoch 12/20, Loss : 1.3646897077560425\n",
      "Epoch 13/20, Loss : 1.3623602390289307\n",
      "Epoch 14/20, Loss : 1.3603529930114746\n",
      "Epoch 15/20, Loss : 1.3585747480392456\n",
      "Epoch 16/20, Loss : 1.3569780588150024\n",
      "Epoch 17/20, Loss : 1.3555351495742798\n",
      "Epoch 18/20, Loss : 1.35422682762146\n",
      "Epoch 19/20, Loss : 1.3530385494232178\n",
      "Epoch 20/20, Loss : 1.35195791721344\n"
     ]
    }
   ],
   "source": [
    "# we will use batch gradient descent (BGD) , using the full training set at each training step\n",
    "\n",
    "learning_rate = 0.4\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = X_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        b -= learning_rate * b.grad\n",
    "        w -= learning_rate * w.grad\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e3c37d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1702],\n",
       "        [2.0141],\n",
       "        [2.0942]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3] # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = X_new @ w + b # use the trained parameters to make predictions\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c091f40",
   "metadata": {},
   "source": [
    "## **Linear Regeression using pytorch's high - level API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2ea35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(in_features= n_features , out_features= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d488c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3117], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee705fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad294f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdbb0bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4718],\n",
       "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c4dfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c51cddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bgd(model , optimizer , criterion , X_train , y_train , n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred , y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1} / {n_epochs} , Loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624cd2e",
   "metadata": {},
   "source": [
    "**we are now using higher level constructs rather than working directly with tensors and autograd.**\n",
    "- **in pytorch the loss funtion object is commonly referred to as criterion, to distinguish it from the loss value itslef. in this example, its ther MSELoss instance.**\n",
    "- **the ``optimizer.step()`` line corresponds to the two lines that updated b and w in our earlier code**\n",
    "- **the ``optimizer.zero_grad()`` line corresponds to the twwo lines that zeroed out b.grad and w.grad.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a978cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([11610])) that is different to the input size (torch.Size([11610, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 4.795647144317627\n",
      "Epoch 2 / 20 , Loss : 1.513105034828186\n",
      "Epoch 3 / 20 , Loss : 1.3611774444580078\n",
      "Epoch 4 / 20 , Loss : 1.3481870889663696\n",
      "Epoch 5 / 20 , Loss : 1.3454773426055908\n",
      "Epoch 6 / 20 , Loss : 1.3444790840148926\n",
      "Epoch 7 / 20 , Loss : 1.3439384698867798\n",
      "Epoch 8 / 20 , Loss : 1.3435535430908203\n",
      "Epoch 9 / 20 , Loss : 1.3432371616363525\n",
      "Epoch 10 / 20 , Loss : 1.3429617881774902\n",
      "Epoch 11 / 20 , Loss : 1.3427164554595947\n",
      "Epoch 12 / 20 , Loss : 1.3424957990646362\n",
      "Epoch 13 / 20 , Loss : 1.3422966003417969\n",
      "Epoch 14 / 20 , Loss : 1.342116355895996\n",
      "Epoch 15 / 20 , Loss : 1.3419532775878906\n",
      "Epoch 16 / 20 , Loss : 1.341805338859558\n",
      "Epoch 17 / 20 , Loss : 1.3416712284088135\n",
      "Epoch 18 / 20 , Loss : 1.3415493965148926\n",
      "Epoch 19 / 20 , Loss : 1.3414386510849\n",
      "Epoch 20 / 20 , Loss : 1.3413382768630981\n"
     ]
    }
   ],
   "source": [
    "# lets call this function to train our model\n",
    "\n",
    "train_bgd(model , optimizer , mse , X_train , y_train , n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7de17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28b8c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0848],\n",
       "        [2.0777],\n",
       "        [2.1337]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774976c4",
   "metadata": {},
   "source": [
    "## **Implementing a Regression MLP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "215e7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features , 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50 , 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "333aecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 5.1346917152404785\n",
      "Epoch 2 / 20 , Loss : 2.4127113819122314\n",
      "Epoch 3 / 20 , Loss : 1.5516926050186157\n",
      "Epoch 4 / 20 , Loss : 1.4793435335159302\n",
      "Epoch 5 / 20 , Loss : 1.4433225393295288\n",
      "Epoch 6 / 20 , Loss : 1.4220961332321167\n",
      "Epoch 7 / 20 , Loss : 1.4084908962249756\n",
      "Epoch 8 / 20 , Loss : 1.3992727994918823\n",
      "Epoch 9 / 20 , Loss : 1.392701506614685\n",
      "Epoch 10 / 20 , Loss : 1.387839674949646\n",
      "Epoch 11 / 20 , Loss : 1.3841078281402588\n",
      "Epoch 12 / 20 , Loss : 1.381138563156128\n",
      "Epoch 13 / 20 , Loss : 1.3787033557891846\n",
      "Epoch 14 / 20 , Loss : 1.376653790473938\n",
      "Epoch 15 / 20 , Loss : 1.374885082244873\n",
      "Epoch 16 / 20 , Loss : 1.373327612876892\n",
      "Epoch 17 / 20 , Loss : 1.3719358444213867\n",
      "Epoch 18 / 20 , Loss : 1.3706709146499634\n",
      "Epoch 19 / 20 , Loss : 1.3695083856582642\n",
      "Epoch 20 / 20 , Loss : 1.3684359788894653\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "train_bgd(model , optimizer , mse , X_train , y_train , n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55425464",
   "metadata": {},
   "source": [
    "## **Implementing Mini-Batch Gradient Decent Using DataLoaders**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12594d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train , y_train)\n",
    "train_loader = DataLoader(train_dataset , batch_size = 32 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e24e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features , 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50 , 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40 , 1)\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce056abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a train() function to implement mini batch GD\n",
    "\n",
    "def train(model , optimizer , criterion , train_loader , n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        for X_batch , y_batch in train_loader:\n",
    "            X_batch , y_batch = X_batch.to(device) , y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred , y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} / {n_epochs}, Loss : {mean_loss : 4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a89885cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 40, Loss :  5.132704\n",
      "Epoch 2 / 40, Loss :  5.130645\n",
      "Epoch 3 / 40, Loss :  5.131664\n",
      "Epoch 4 / 40, Loss :  5.132036\n",
      "Epoch 5 / 40, Loss :  5.131548\n",
      "Epoch 6 / 40, Loss :  5.131827\n",
      "Epoch 7 / 40, Loss :  5.131869\n",
      "Epoch 8 / 40, Loss :  5.131220\n",
      "Epoch 9 / 40, Loss :  5.132124\n",
      "Epoch 10 / 40, Loss :  5.131827\n",
      "Epoch 11 / 40, Loss :  5.132171\n",
      "Epoch 12 / 40, Loss :  5.130879\n",
      "Epoch 13 / 40, Loss :  5.132786\n",
      "Epoch 14 / 40, Loss :  5.131691\n",
      "Epoch 15 / 40, Loss :  5.132223\n",
      "Epoch 16 / 40, Loss :  5.132125\n",
      "Epoch 17 / 40, Loss :  5.132180\n",
      "Epoch 18 / 40, Loss :  5.132291\n",
      "Epoch 19 / 40, Loss :  5.131952\n",
      "Epoch 20 / 40, Loss :  5.131891\n",
      "Epoch 21 / 40, Loss :  5.130974\n",
      "Epoch 22 / 40, Loss :  5.131176\n",
      "Epoch 23 / 40, Loss :  5.133286\n",
      "Epoch 24 / 40, Loss :  5.132309\n",
      "Epoch 25 / 40, Loss :  5.131234\n",
      "Epoch 26 / 40, Loss :  5.132150\n",
      "Epoch 27 / 40, Loss :  5.132177\n",
      "Epoch 28 / 40, Loss :  5.131949\n",
      "Epoch 29 / 40, Loss :  5.131540\n",
      "Epoch 30 / 40, Loss :  5.132351\n",
      "Epoch 31 / 40, Loss :  5.131679\n",
      "Epoch 32 / 40, Loss :  5.132048\n",
      "Epoch 33 / 40, Loss :  5.131710\n",
      "Epoch 34 / 40, Loss :  5.132270\n",
      "Epoch 35 / 40, Loss :  5.131857\n",
      "Epoch 36 / 40, Loss :  5.132846\n",
      "Epoch 37 / 40, Loss :  5.132009\n",
      "Epoch 38 / 40, Loss :  5.131647\n",
      "Epoch 39 / 40, Loss :  5.131994\n",
      "Epoch 40 / 40, Loss :  5.131635\n"
     ]
    }
   ],
   "source": [
    "train(model , optimizer , mse , train_loader , 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935917d",
   "metadata": {},
   "source": [
    "## **Model Evaluation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b63dc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model , data_loader , metric_fn , aggregate_fn=torch.mean):\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch , y_batch = X_batch.to(device) , y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric = metric_fn(y_pred, y_batch)\n",
    "            metrics.append(metric)\n",
    "        return aggregate_fn(torch.stack(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3567750c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9528, device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = TensorDataset(X_valid , y_valid)\n",
    "valid_loader = DataLoader(valid_dataset , batch_size = 32)\n",
    "valid_mse = evaluate(model , valid_loader, mse)\n",
    "valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86e84afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2143, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse(y_pred , y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean().sqrt()\n",
    "\n",
    "evaluate(model , valid_loader , rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "627697d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2255, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mse.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e634a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\HOML_pytorch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2255, device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model , valid_loader , mse , aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cad976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model , data_loader , metric):\n",
    "    model.eval()\n",
    "    metric.reset() # resets the metric at the beginning\n",
    "    with torch.no_grad():\n",
    "        for X_batch , y_batch in data_loader:\n",
    "            X_batch , y_batch = X_batch.to(device) , y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred , y_batch) # update it at each iteration\n",
    "\n",
    "    return metric.compute() # compute the final result at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aca6dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = torchmetrics.MeanSquaredError(squared= False).to(device)\n",
    "# evaluate_tm(model , valid_loader , rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583080e5",
   "metadata": {},
   "source": [
    "## **Building non sequential models using custom modules**\n",
    "\n",
    "**One example of nonsequential neural network is a wide and deep neural network.This architecture makes it possible for a neural network to learn both deep patterns and simple rules. The short path can also be used to provide manual features to the neural network. In contrast , a regualr MLP forces all the data to flow through the full stack of layers; thus simple patterns in the data may end up being distorted by the sequence of transformations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self , n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features , 50), nn.ReLU(), \n",
    "            nn.Linear(50 , 40), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(40 + n_features , 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
