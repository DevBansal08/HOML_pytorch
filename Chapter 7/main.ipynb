{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f2fc8e",
   "metadata": {},
   "source": [
    "# <center>**Chapter 7 : Dimensionality Reduction**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65922a",
   "metadata": {},
   "source": [
    "- many ML probelms involve thousands or even millions of features for each training instance which makes it harder and extremely slow to find a good solution. This is called the curse of dimensionality.\n",
    "\n",
    "- **Warning :** reducing dimesionality can also drop some useful information just like compressing an image to JPEG can degerade its quality\n",
    "\n",
    "- apart from speeding up training and possibly improving our model's preformance , dimensionality reduction is also extremely useful in data visualisation. Reducing the number of dimensions down to two or three makes it possible to plot a condensed view of a high dimensional training set on a graph and often gain some important insights by visually detecting patterns such as clusters\n",
    "\n",
    "- in this chapter we'll go through three of the most popular dimensionality reductionn techniques:\n",
    "    1. PCA\n",
    "    2. random projection\n",
    "    3. local linear embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe3b81",
   "metadata": {},
   "source": [
    "## <center>**The curse of Dimensionality**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa563069",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
