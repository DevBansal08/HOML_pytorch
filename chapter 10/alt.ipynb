{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7443d3",
   "metadata": {},
   "source": [
    "# <center>**Chapter 10 : Building Neural Nwtworks with Pytorch**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7253a9",
   "metadata": {},
   "source": [
    "## **Pytorch Fundamentals**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d5fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803cb750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0 , 4.0 , 7.0] , [2.0 , 3.0 , 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53659c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94192c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor([4., 3.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing works just like numpy arrays\n",
    "X[0,1] , X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb3a52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 7.],\n",
       "       [2., 3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77bb493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 7],\n",
       "        [2, 3, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(np.array([[1 , 4 , 7] , [ 2 , 3 , 6]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c7986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch's API also provides many inplace operations\n",
    "\n",
    "X.relu_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5be2c",
   "metadata": {},
   "source": [
    "## **Hardware Acceleration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a587115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ea46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1. , 2. , 3.] , [4.,5.,6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21976ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af52a951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T # this runs on the GPU\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba806b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.83 ms ± 91.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand(1000 , 1000) # on the CPU\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405ae61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 μs ± 7.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000) , device = \"cuda\")\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002f547",
   "metadata": {},
   "source": [
    "## **Autograd**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef7d400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "f = x ** 2\n",
    "f\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1122a5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d57a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad  # this is the gradient decent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to avoid gradient computation\n",
    "\n",
    "# detach method createds a new tensor detached from the computation graph with requires_grad = False, but still pointing to same data in memory\n",
    "# this can be effective when you need to run some computations on a tensor without affecting the gradients ( eg : evaluation) , or when you need fine grained control over which operations should contribute to gradient computation.\n",
    "\n",
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95778ce",
   "metadata": {},
   "source": [
    "### **Warning : If you forget to zero out the gradients at each training iteration, the backward() method will just accumulate them, causing incorrect gradient descent updates. Since, there wont be any explicit error, just low performance this issue may be hard to debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86147af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "367f8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting  everything together, the whole training loop looks like this:\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0 , requires_grad= True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2\n",
    "    f.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # this is the gradient descent step\n",
    "\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974cf8d",
   "metadata": {},
   "source": [
    "#### 1.  **Some oprerations - such as exp(), relu(), rsqrt(), sigmoid(), sqrt(), tan(), and tanh() - save their outputs in the computation graph during the forward pass, then use these outputs to compute the gradients during the backward pass. This means that you must not modify such an operations output in place, or you will get an error during the backward pass**\n",
    "\n",
    "#### 2.  **Other operations  such as abs(), cos() , log() , sin(), square(), and var() save their inputs instead of their oputputs. Such an operation doesnot care if you modify its output in place, but you must not modify its inputs in place before the backward pass.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e211b",
   "metadata": {},
   "source": [
    "## **Implementing Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a344528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
