{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7443d3",
   "metadata": {},
   "source": [
    "# <center>**Chapter 10 : Building Neural Nwtworks with Pytorch**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7253a9",
   "metadata": {},
   "source": [
    "## **Pytorch Fundamentals**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d5fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803cb750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0 , 4.0 , 7.0] , [2.0 , 3.0 , 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53659c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94192c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor([4., 3.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing works just like numpy arrays\n",
    "X[0,1] , X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb3a52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 7.],\n",
       "       [2., 3., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77bb493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 7],\n",
       "        [2, 3, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(np.array([[1 , 4 , 7] , [ 2 , 3 , 6]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c7986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch's API also provides many inplace operations\n",
    "\n",
    "X.relu_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5be2c",
   "metadata": {},
   "source": [
    "## **Hardware Acceleration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a587115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ea46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1. , 2. , 3.] , [4.,5.,6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21976ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af52a951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T # this runs on the GPU\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba806b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.27 ms ± 855 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand(1000 , 1000) # on the CPU\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405ae61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 μs ± 83.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000 , 1000) , device = \"cuda\")\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002f547",
   "metadata": {},
   "source": [
    "## **Autograd**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef7d400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0 , requires_grad=True)\n",
    "f = x ** 2\n",
    "f\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1122a5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d57a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad  # this is the gradient decent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4f0eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to avoid gradient computation\n",
    "\n",
    "# detach method createds a new tensor detached from the computation graph with requires_grad = False, but still pointing to same data in memory\n",
    "# this can be effective when you need to run some computations on a tensor without affecting the gradients ( eg : evaluation) , or when you need fine grained control over which operations should contribute to gradient computation.\n",
    "\n",
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95778ce",
   "metadata": {},
   "source": [
    "### **Warning : If you forget to zero out the gradients at each training iteration, the backward() method will just accumulate them, causing incorrect gradient descent updates. Since, there wont be any explicit error, just low performance this issue may be hard to debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86147af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367f8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting  everything together, the whole training loop looks like this:\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0 , requires_grad= True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2\n",
    "    f.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # this is the gradient descent step\n",
    "\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974cf8d",
   "metadata": {},
   "source": [
    "#### 1.  **Some oprerations - such as exp(), relu(), rsqrt(), sigmoid(), sqrt(), tan(), and tanh() - save their outputs in the computation graph during the forward pass, then use these outputs to compute the gradients during the backward pass. This means that you must not modify such an operations output in place, or you will get an error during the backward pass**\n",
    "\n",
    "#### 2.  **Other operations  such as abs(), cos() , log() , sin(), square(), and var() save their inputs instead of their oputputs. Such an operation doesnot care if you modify its output in place, but you must not modify its inputs in place before the backward pass.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e211b",
   "metadata": {},
   "source": [
    "## **Implementing Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a344528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full , X_test ,y_train_full , y_test = train_test_split(housing.data , housing.target , random_state = 42)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full , y_train_full , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e351ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to tensors\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "mean = X_train.mean(dim = 0 , keepdims= True)\n",
    "stds = X_train.std(dim = 0 , keepdims= True)\n",
    "X_train = (X_train - mean)/stds\n",
    "X_valid = (X_valid - mean)/stds\n",
    "X_test = (X_test - mean)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e305acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the target column to tensors\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1,1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9651bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of our linear regression model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_features = X_train.shape[1] # there are 8 input features\n",
    "w = torch.randn((n_features , 1), requires_grad=True)\n",
    "b = torch.tensor(0. , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a308a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 , Loss: 16.158456802368164\n",
      "Epoch 2/20 , Loss: 4.879366397857666\n",
      "Epoch 3/20 , Loss: 2.2552270889282227\n",
      "Epoch 4/20 , Loss: 1.3307628631591797\n",
      "Epoch 5/20 , Loss: 0.9680694937705994\n",
      "Epoch 6/20 , Loss: 0.8142679929733276\n",
      "Epoch 7/20 , Loss: 0.7417047023773193\n",
      "Epoch 8/20 , Loss: 0.7020702362060547\n",
      "Epoch 9/20 , Loss: 0.6765919923782349\n",
      "Epoch 10/20 , Loss: 0.6577966213226318\n",
      "Epoch 11/20 , Loss: 0.6426153182983398\n",
      "Epoch 12/20 , Loss: 0.6297224760055542\n",
      "Epoch 13/20 , Loss: 0.6184942722320557\n",
      "Epoch 14/20 , Loss: 0.608596920967102\n",
      "Epoch 15/20 , Loss: 0.5998217463493347\n",
      "Epoch 16/20 , Loss: 0.5920187830924988\n",
      "Epoch 17/20 , Loss: 0.5850691795349121\n",
      "Epoch 18/20 , Loss: 0.578873336315155\n",
      "Epoch 19/20 , Loss: 0.573345422744751\n",
      "Epoch 20/20 , Loss: 0.5684100985527039\n"
     ]
    }
   ],
   "source": [
    "# using batch gradient descent\n",
    "\n",
    "learning_rate = 0.4   # first we define the learning rate\n",
    "n_epochs = 20    # run the loop for 20 epochs\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = X_train @ w + b    # running the forward pass\n",
    "    loss = ((y_pred - y_train) ** 2).mean()  # calculating the mean squared error\n",
    "    loss.backward()  # we run this to compute the gradients of the loss\n",
    "    with torch.no_grad():   # performing the gradient descent step\n",
    "        b -= learning_rate * b.grad\n",
    "        w -= learning_rate * w.grad\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} , Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f59acb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916],\n",
       "        [1.6480],\n",
       "        [2.6577]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model on new unseen data\n",
    "\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = X_new @ w + b # using the trained parameter to make pedictions\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503044e2",
   "metadata": {},
   "source": [
    "### **TIP : It's best to use ```torch.no_grad``` context during inference. Pytorch will consume less RAM and run faster since it wont have to keep track of the computation graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22b229",
   "metadata": {},
   "source": [
    "## **Linear Regression using pytorch's High level API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4261e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # conventional way to import the model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(in_features=n_features , out_features= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "446002cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3117], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b57c6737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86919d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.named_parameters():\n",
    "    [...]  # do something with each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce5149a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4718],\n",
       "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e9cc4",
   "metadata": {},
   "source": [
    "#### **when we use a module as a function, pytorch internally calls the module's ```forward()``` method. in the case of the ```nn.Linear``` module, the forward() method computes X @ self.weight.T + self.bias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57303cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have our model, we need to create an optimizer to update the model parameters, and we must also choose a loss function\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63dcb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small function to train our model\n",
    "\n",
    "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} , Loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86ebdb",
   "metadata": {},
   "source": [
    "- ##### **in pytorch the loss function object is commonly referred to as the criterion, to distinguish it from the loss value itself.**\n",
    "- ##### **the ```optimizer.step()``` line corresponds to the two lines that updated b and w in our earlier code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efe20328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 , Loss : 4.3378496170043945\n",
      "Epoch 2/20 , Loss : 0.780293345451355\n",
      "Epoch 3/20 , Loss : 0.6253840327262878\n",
      "Epoch 4/20 , Loss : 0.6060433387756348\n",
      "Epoch 5/20 , Loss : 0.5956299304962158\n",
      "Epoch 6/20 , Loss : 0.5873566269874573\n",
      "Epoch 7/20 , Loss : 0.5802990198135376\n",
      "Epoch 8/20 , Loss : 0.5741382241249084\n",
      "Epoch 9/20 , Loss : 0.5687100887298584\n",
      "Epoch 10/20 , Loss : 0.5639079213142395\n",
      "Epoch 11/20 , Loss : 0.5596510767936707\n",
      "Epoch 12/20 , Loss : 0.5558737516403198\n",
      "Epoch 13/20 , Loss : 0.5525193810462952\n",
      "Epoch 14/20 , Loss : 0.5495391488075256\n",
      "Epoch 15/20 , Loss : 0.5468899011611938\n",
      "Epoch 16/20 , Loss : 0.544533908367157\n",
      "Epoch 17/20 , Loss : 0.5424376130104065\n",
      "Epoch 18/20 , Loss : 0.5405715703964233\n",
      "Epoch 19/20 , Loss : 0.5389096736907959\n",
      "Epoch 20/20 , Loss : 0.5374288558959961\n"
     ]
    }
   ],
   "source": [
    "# function call to train our model\n",
    "\n",
    "train_bgd(model, optimizer, mse, X_train , y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e0da4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8061],\n",
       "        [1.7116],\n",
       "        [2.6973]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[0:3]    # pretend these are new instances\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_new)    # use the trained model to make predictions\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026636e8",
   "metadata": {},
   "source": [
    "##  **Implementing a Regression MLP**\n",
    "\n",
    "- ##### **pytorch provides a helpful ```nn.Sequential``` module that chains multiple modules: when you call this module with some inputs, it feeds these inputs to the first module, then feeds the output of the first module to the second module and so on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df96b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features,50),      # n_features = 8, number of outputs = 50\n",
    "    nn.ReLU(),                     # ReLu activation function\n",
    "    nn.Linear(50,40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09175d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 , Loss : 5.045480251312256\n",
      "Epoch 2/20 , Loss : 2.0523128509521484\n",
      "Epoch 3/20 , Loss : 1.0039883852005005\n",
      "Epoch 4/20 , Loss : 0.8570139408111572\n",
      "Epoch 5/20 , Loss : 0.7740675210952759\n",
      "Epoch 6/20 , Loss : 0.7225848436355591\n",
      "Epoch 7/20 , Loss : 0.6893726587295532\n",
      "Epoch 8/20 , Loss : 0.6669033765792847\n",
      "Epoch 9/20 , Loss : 0.6507738828659058\n",
      "Epoch 10/20 , Loss : 0.6383934617042542\n",
      "Epoch 11/20 , Loss : 0.6281994581222534\n",
      "Epoch 12/20 , Loss : 0.6193399429321289\n",
      "Epoch 13/20 , Loss : 0.6113173365592957\n",
      "Epoch 14/20 , Loss : 0.6038705706596375\n",
      "Epoch 15/20 , Loss : 0.5968308448791504\n",
      "Epoch 16/20 , Loss : 0.5901119112968445\n",
      "Epoch 17/20 , Loss : 0.5836468935012817\n",
      "Epoch 18/20 , Loss : 0.5774064064025879\n",
      "Epoch 19/20 , Loss : 0.5713555216789246\n",
      "Epoch 20/20 , Loss : 0.565444827079773\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "train_bgd(model , optimizer , mse , X_train, y_train , n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87717466",
   "metadata": {},
   "source": [
    "## **Implementing Mini-Batch Gradient Descent using Dataloaders**\n",
    "\n",
    "- ##### **to implement mini batch GD, pytorch provides a class named Dataloader. It can efficiently load batches of data of the desired size and shuffle the data at each epoch if we want to**\n",
    "- ##### **we first need to wrap X_train and y_train tensors in a dataset object with the required API, to help with this, pytorch provides a TensorDataset class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "067cfc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train , y_train)\n",
    "train_loader = DataLoader(train_dataset , batch_size= 32 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07fbf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50 , 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,1)\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef172b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N lets create a train function to implement mini batch GD\n",
    "\n",
    "def train(model, optimizer , critereion , train_loader , n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        for X_batch , y_batch in train_loader:\n",
    "            X_batch , y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = critereion(y_pred , y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} , Loss : {mean_loss : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7ae420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 , Loss :  5.0443\n",
      "Epoch 2/20 , Loss :  5.0456\n",
      "Epoch 3/20 , Loss :  5.0457\n",
      "Epoch 4/20 , Loss :  5.0450\n",
      "Epoch 5/20 , Loss :  5.0452\n",
      "Epoch 6/20 , Loss :  5.0449\n",
      "Epoch 7/20 , Loss :  5.0449\n",
      "Epoch 8/20 , Loss :  5.0459\n",
      "Epoch 9/20 , Loss :  5.0460\n",
      "Epoch 10/20 , Loss :  5.0456\n",
      "Epoch 11/20 , Loss :  5.0449\n",
      "Epoch 12/20 , Loss :  5.0461\n",
      "Epoch 13/20 , Loss :  5.0452\n",
      "Epoch 14/20 , Loss :  5.0456\n",
      "Epoch 15/20 , Loss :  5.0454\n",
      "Epoch 16/20 , Loss :  5.0455\n",
      "Epoch 17/20 , Loss :  5.0460\n",
      "Epoch 18/20 , Loss :  5.0451\n",
      "Epoch 19/20 , Loss :  5.0456\n",
      "Epoch 20/20 , Loss :  5.0445\n"
     ]
    }
   ],
   "source": [
    "train(model , optimizer , mse , train_loader , n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6be07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
